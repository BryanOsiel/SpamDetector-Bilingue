# type: ignore

"""
SpamDetector BILING√úE (ES+EN) - compatible con Python 3.8 y TensorFlow 2.13.0
Entrena, eval√∫a y guarda un modelo LSTM Bidireccional para detecci√≥n de spam.
"""

# -------------------------------------------------------
# 1Ô∏è‚É£ Importaciones
# -------------------------------------------------------
import os
import re
import json
import numpy as np
import pandas as pd
import tensorflow as tf
from tensorflow.keras.preprocessing.text import Tokenizer
from tensorflow.keras.preprocessing.sequence import pad_sequences
from tensorflow.keras.models import Sequential, load_model
from tensorflow.keras.layers import Embedding, Bidirectional, LSTM, Dense, Dropout, Input
from tensorflow.keras.callbacks import EarlyStopping
from sklearn.model_selection import train_test_split
import requests
import zipfile
import io # Necesario para leer el CSV de espa√±ol desde la URL

# -------------------------------------------------------
# 2Ô∏è‚É£ Carga y limpieza de datasets (Ingl√©s + Espa√±ol)
# -------------------------------------------------------

def limpiar_texto(texto):
    """Funci√≥n de limpieza mejorada para incluir caracteres en espa√±ol"""
    texto = texto.lower()
    texto = re.sub(r"http\S+|www\S+", "", texto)
    # Permitimos letras, n√∫meros, espacios, ap√≥strofes y caracteres en espa√±ol
    texto = re.sub(r"[^a-zA-Z0-9\s'√°√©√≠√≥√∫√º√±]", "", texto) 
    texto = re.sub(r'\s+', ' ', texto).strip()
    return texto

# --- Cargar datos en INGL√âS (el original) ---
print("üì• Descargando dataset en Ingl√©s (sms.tsv)...")
url_en = "https://raw.githubusercontent.com/justmarkham/DAT8/master/data/sms.tsv"
data_en = pd.read_csv(url_en, sep='\t', header=None, names=['label', 'message'])

# --- Cargar datos en ESPA√ëOL ---
print("üì• Descargando dataset en Espa√±ol (corpus-spanish-v1.csv)...")
url_es = "https://raw.githubusercontent.com/johfl/sms-spam-corpus-spanish/master/corpus-spanish-v1.csv"
try:
    r_es = requests.get(url_es).content
    # Usamos 'latin1' ya que este CSV en particular lo requiere
    
    # üî¥ ARREGLO 1: A√±adido sep=';' para leer el CSV correctamente
    data_es = pd.read_csv(io.StringIO(r_es.decode('latin1')), sep=';', usecols=['label', 'text'])
    
    data_es.rename(columns={'text': 'message'}, inplace=True) # Renombrar para que coincida
except Exception as e:
    print(f"Error al descargar o leer el dataset en espa√±ol: {e}")
    data_es = pd.DataFrame(columns=['label', 'message']) # Crear DF vac√≠o si falla

# --- Combinar y limpiar datasets ---
data = pd.concat([data_en, data_es], ignore_index=True)
data.dropna(inplace=True) # Quitar filas nulas que puedan venir del merge
data['message'] = data['message'].astype(str).apply(limpiar_texto)
data['label'] = data['label'].map({'ham': 0, 'spam': 1})

# Eliminar duplicados que puedan surgir de la limpieza
data = data.drop_duplicates()
# Asegurarnos de que no haya NaNs en la etiqueta despu√©s del map
data.dropna(subset=['label'], inplace=True)
data['label'] = data['label'].astype(int)

print(f"\nTotal de mensajes (EN+ES): {len(data)}")
print(data['label'].value_counts(dropna=False))


# -------------------------------------------------------
# 3Ô∏è‚É£ Tokenizaci√≥n y padding (Sin cambios)
# -------------------------------------------------------
# Aumentamos el vocabulario para dos idiomas
num_words = 20000 
tokenizer = Tokenizer(num_words=num_words, oov_token='<OOV>')
tokenizer.fit_on_texts(data['message'])
sequences = tokenizer.texts_to_sequences(data['message'])

max_length = 120
X = pad_sequences(sequences, maxlen=max_length, padding='post', truncating='post')
y = np.array(data['label'])

X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.2, random_state=42)

# -------------------------------------------------------
# 4Ô∏è‚É£ Cargar embeddings fastText (MUSE Aligned)
# -------------------------------------------------------
# fastText usa 300 dimensiones
embedding_dim = 300 
EMBEDDING_FILE_ES = "wiki.es.align.vec"
EMBEDDING_FILE_EN = "wiki.en.align.vec"

if not os.path.exists(EMBEDDING_FILE_ES) or not os.path.exists(EMBEDDING_FILE_EN):
    print("="*50)
    print("üö® ¬°ERROR: ARCHIVOS DE EMBEDDINGS NO ENCONTRADOS!")
    print(f"Por favor, descarga 'wiki.es.align.vec' y 'wiki.en.align.vec'")
    print("Desde: https://fasttext.cc/docs/en/aligned-vectors.html")
    print("Y col√≥calos en la misma carpeta que este script.")
    print("="*50)
    exit()

print("üß† Cargando embeddings de fastText (puede tardar)...")
embeddings_index = {}

# üî¥ ARREGLO 2: Funci√≥n 'load_fasttext_vectors' actualizada
def load_fasttext_vectors(filepath):
    """Carga los vectores de fastText manejando palabras con espacios."""
    print(f"Cargando vectores desde {filepath}...")
    with open(filepath, encoding='utf8') as f:
        # La primera l√≠nea de fastText es un header (vocab_size, dim), la saltamos
        next(f) 
        for line in f:
            values = line.split()
            
            if len(values) < (embedding_dim + 1):
                # Ignorar l√≠neas malformadas que no tienen suficientes valores
                continue

            # La 'palabra' es todo MENOS los √∫ltimos 'embedding_dim' (300) valores
            word_parts = values[:-embedding_dim]
            word = " ".join(word_parts)
            
            # Los 'coefs' son SOLAMENTE los √∫ltimos 'embedding_dim' (300) valores
            coefs_str = values[-embedding_dim:]

            if word in tokenizer.word_index: # Optimiz: solo cargar palabras que est√°n en nuestro corpus
                try:
                    coefs = np.asarray(coefs_str, dtype='float32')
                    embeddings_index[word] = coefs
                except ValueError:
                    # Ocasionalmente, una l√≠nea puede estar corrupta. La saltamos.
                    pass
                    
    print(f"‚úÖ Vectores cargados desde {filepath}")

# Cargar ambos archivos en el mismo diccionario
load_fasttext_vectors(EMBEDDING_FILE_ES)
load_fasttext_vectors(EMBEDDING_FILE_EN)

print(f"Total de vectores de palabras cargados: {len(embeddings_index)}")

# --- Crear la Matriz de Embeddings ---
word_index = tokenizer.word_index
embedding_matrix = np.zeros((num_words, embedding_dim))
palabras_encontradas = 0

for word, i in word_index.items():
    if i >= num_words:
        continue
    embedding_vector = embeddings_index.get(word)
    if embedding_vector is not None:
        # Palabras encontradas en fastText
        embedding_matrix[i] = embedding_vector
        palabras_encontradas += 1

print(f"Palabras encontradas en fastText: {palabras_encontradas} / {min(num_words, len(word_index))}")


# -------------------------------------------------------
# 5Ô∏è‚É£ Modelo con LSTM Bidireccional
# -------------------------------------------------------
model = Sequential([
    Input(shape=(max_length,)),
    Embedding(
        input_dim=num_words, # 20000
        output_dim=embedding_dim, # 300
        weights=[embedding_matrix],
        input_length=max_length, 
        trainable=True # Permitimos fine-tuning
    ),
    Bidirectional(LSTM(128, return_sequences=True, dropout=0.3, recurrent_dropout=0.3)),
    Bidirectional(LSTM(64, dropout=0.3)),
    Dense(64, activation='relu'),
    Dropout(0.4),
    Dense(1, activation='sigmoid')
])

optimizer = tf.keras.optimizers.Adam(learning_rate=1e-4)
model.compile(loss='binary_crossentropy', optimizer=optimizer, metrics=['accuracy'])
model.summary()

# -------------------------------------------------------
# 6Ô∏è‚É£ Entrenamiento con EarlyStopping (Sin cambios)
# -------------------------------------------------------
early_stop = EarlyStopping(monitor='val_loss', patience=3, restore_best_weights=True)

history = model.fit(
    X_train, y_train,
    epochs=20,
    batch_size=64,
    validation_split=0.2,
    callbacks=[early_stop],
    verbose=1
)

# -------------------------------------------------------
# 7Ô∏è‚É£ Evaluaci√≥n (Sin cambios)
# -------------------------------------------------------
loss, accuracy = model.evaluate(X_test, y_test, verbose=0)
print(f"\nüìä Precisi√≥n en datos de prueba: {accuracy * 100:.2f}%")

# -------------------------------------------------------
# 8Ô∏è‚É£ Funci√≥n para predecir mensajes nuevos
# -------------------------------------------------------
def predecir_mensaje(texto, threshold=0.5):
    texto = limpiar_texto(texto)
    seq = tokenizer.texts_to_sequences([texto])
    pad = pad_sequences(seq, maxlen=max_length, padding='post', truncating='post')
    pred = model.predict(pad, verbose=0)[0][0]
    return f"üö® SPAM ({pred:.2f})" if pred > threshold else f"‚úÖ NO SPAM ({pred:.2f})"

# -------------------------------------------------------
# 9Ô∏è‚É£ Guardar modelo y tokenizer (VERSI√ìN CORREGIDA)
# -------------------------------------------------------
model.save("spam_model_bilingue.keras")
print("‚úÖ Modelo guardado como spam_model_bilingue.keras")

# --- CAMBIO ---
# Usamos el m√©todo to_json() del tokenizer, que guarda la config completa
print("Guardando tokenizer...")
tokenizer_json = tokenizer.to_json()
with open('tokenizer_config_bilingue.json', 'w', encoding='utf8') as f:
    f.write(tokenizer_json)
print("‚úÖ Tokenizer guardado como tokenizer_config_bilingue.json")

# -------------------------------------------------------
# üîü Modo interactivo (Sin cambios)
# -------------------------------------------------------
if __name__ == "__main__":
    print("\nüí¨ Modo interactivo biling√ºe iniciado (escribe 'salir' para terminar)")
    while True:
        mensaje = input("\n‚úâÔ∏è Escribe un mensaje para analizar (EN o ES):\n> ")
        if mensaje.lower().strip() == "salir":
            print("üëã ¬°Hasta luego!")
            break
        print("Predicci√≥n:", predecir_mensaje(mensaje))